{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "\n",
    "def calculate_error(y, y_pred, w):\n",
    "    #TODO : Calculate the weighted error of a weak classifier.\n",
    "\n",
    "     return (sum(w * (np.not_equal(y, y_pred)).astype(int)))/sum(w)\n",
    "\n",
    "def calculate_alpha(error):\n",
    "    #TODO : Calculate the weight of a weak classifier.\n",
    "    return np.log((1-error)/error)/2\n",
    "\n",
    "def update_weights(w, alpha, y, y_pred):\n",
    "    #TODO : Update weights after a boosting iteration.\n",
    "    return w*np.exp(alpha*(np.not_equal(y,y_pred)).astype(int))\n",
    "\n",
    "\n",
    "def classify(X, y, maxitercnt=10000):\n",
    "    rows, cols = X.shape\n",
    "    v = np.zeros(cols + 1)  # Initialize weights (including bias)\n",
    "    w = np.zeros(cols + 1)  # Best weights found\n",
    "    best_accuracy = 0\n",
    "    ones_column = np.ones((X.shape[0], 1))\n",
    "    new_X = np.hstack((ones_column, X))  # Add bias term\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    for itercnt in range(maxitercnt):\n",
    "        random_index = np.random.choice(rows)\n",
    "        xj, yj = new_X[random_index], y[random_index]\n",
    "        yhat = np.sign(np.dot(v, xj))\n",
    "\n",
    "        # Update weights if there's a misclassification\n",
    "        if yhat != yj:\n",
    "            v += yj * xj\n",
    "\n",
    "        # Evaluate current weight vector on the full dataset\n",
    "        predictions = np.sign(np.dot(new_X, v))\n",
    "        accuracy = np.mean(predictions == y)\n",
    "\n",
    "        # If this weight vector performs better, save it\n",
    "        if accuracy > best_accuracy:\n",
    "            w = v.copy()\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def predict(X, weights):\n",
    "    arr = np.dot(X, weights[1:]) + weights[0]\n",
    "    return np.sign(arr)\n",
    "\n",
    "\n",
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alphas = []\n",
    "        self.G_M = []\n",
    "        self.training_errors = []\n",
    "\n",
    "    def fit(self, X, y, M = 100):\n",
    "        \n",
    "        self.alphas = [] \n",
    "        self.training_errors = []\n",
    "        self.M = M\n",
    "\n",
    "        for m in range(0, M):\n",
    "            \n",
    "            # Set weights\n",
    "            if m == 0:\n",
    "                #TODO\n",
    "                w=np.ones(len(y))*1/len(y)\n",
    "\n",
    "                 \n",
    "            else:\n",
    "                #TODO\n",
    "                w=update_weights(w,alpha_m,y,y_pred)\n",
    "               \n",
    "            \n",
    "            # 1. Fit weak classifier and predict labels(using predict method) and Save it to list of weak classifiers.\n",
    "            #TODO\n",
    "            #simple tree with max depth 1\n",
    "            G_m=DecisionTreeClassifier(max_depth=1)\n",
    "            #fit\n",
    "            G_m.fit(X,y,w)\n",
    "            \n",
    "            y_pred=G_m.predict(X)\n",
    "            #add weak learner to the list of weak learners\n",
    "            self.G_M.append(G_m)\n",
    "\n",
    "\n",
    "            # 2. Calculate error of this weak classifier and save it to list of trainingterrors.\n",
    "            #TODO\n",
    "            \n",
    "            error_m=calculate_error(y,y_pred,w)\n",
    "            self.training_errors.append(error_m)\n",
    "\n",
    "            # 3. Calculate alpha of this weak classifier and save it to list of alphas.\n",
    "            #TODO\n",
    "            alpha_m=calculate_alpha(error_m)\n",
    "            self.alphas.append(alpha_m)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.M)) \n",
    "\n",
    "        for m in range(self.M):\n",
    "            y_pred_m = self.G_M[m].predict(X) * self.alphas[m]\n",
    "            weak_preds[weak_preds.columns[m]] = y_pred_m\n",
    "\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "banana = np.loadtxt('banana_data.csv', delimiter=',')\n",
    "X_banana, y_banana = banana[:, 1:], banana[:, 0]\n",
    "# X_banana_train, y_banana_train = X_banana[:400], y_banana[:400]\n",
    "# X_banana_test, y_banana_test = X_banana[400:], y_banana[400:]\n",
    "X_banana_train, X_banana_test, y_banana_train, y_banana_test = train_test_split(X_banana, y_banana, test_size=4900, train_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "splice = np.loadtxt('splice_data.csv', delimiter=',')\n",
    "X_splice, y_splice = splice[:, 1:], splice[:, 0]\n",
    "# X_splice_train, X_splice_test, y_splice_train, y_splice_test = train_test_split(X_splice, y_splice, test_size=1991, train_size=1000)\n",
    "\n",
    "X_splice, y_splice = splice[:, 1:], splice[:, 0]\n",
    "X_splice_train, y_splice_train = X_splice[:1000], y_splice[:1000]\n",
    "X_splice_test, y_splice_test = X_splice[1000:], y_splice[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7228571428571429\n",
      "0.7204870301746956\n",
      "0.6211775445002282\n",
      "0.667156862745098\n"
     ]
    }
   ],
   "source": [
    "#TODO : Fit model\n",
    "model=AdaBoost()\n",
    "model.fit(X_banana_train, y_banana_train, 1000)\n",
    "\n",
    "#TODO : Predict on test data\n",
    "y_pred=model.predict(X_banana_test)\n",
    "\n",
    "#TODO : Print evaluation metrics (Accurcy, Precission, Recall and f-score)\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "accuracy=metrics.accuracy_score(y_banana_test, y_pred)\n",
    "precission=metrics.precision_score(y_banana_test,y_pred)\n",
    "recall=metrics.recall_score(y_banana_test,y_pred)\n",
    "f1=metrics.f1_score(y_banana_test,y_pred)\n",
    "print(accuracy)\n",
    "print(precission)\n",
    "print(recall)\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9271722752385736\n",
      "0.9404630650496141\n",
      "0.9036016949152542\n",
      "0.9216639654240951\n"
     ]
    }
   ],
   "source": [
    "model=AdaBoost()\n",
    "model.fit(X_splice_train, y_splice_train, 1000)\n",
    "\n",
    "#TODO : Predict on test data\n",
    "y_pred=model.predict(X_splice_test)\n",
    "\n",
    "#TODO : Print evaluation metrics (Accurcy, Precission, Recall and f-score)\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "accuracy=metrics.accuracy_score(y_splice_test, y_pred)\n",
    "precission=metrics.precision_score(y_splice_test,y_pred)\n",
    "recall=metrics.recall_score(y_splice_test,y_pred)\n",
    "f1=metrics.f1_score(y_splice_test,y_pred)\n",
    "print(accuracy)\n",
    "print(precission)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7825213460572576"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine = DecisionTreeClassifier(max_depth=1)\n",
    "w=np.ones(len(y_splice_train))*1/len(y_splice_train)\n",
    "mine.fit(X_splice_train, y_splice_train, w)\n",
    "y_pred = mine.predict(X_splice_test)\n",
    "metrics.accuracy_score(y_splice_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5826530612244898"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine = DecisionTreeClassifier(max_depth=1)\n",
    "w=np.ones(len(y_banana_train))*1/len(y_banana)\n",
    "mine.fit(X_banana_train, y_banana_train, w)\n",
    "y_pred = mine.predict(X_banana_test)\n",
    "metrics.accuracy_score(y_banana_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
